\documentclass[11pt]{article}
\usepackage{project501}

\begin{document}

\section{Background in Hermitian Geometry}

In this section, we'll define several items:
\begin{itemize}
\item Let $A$ be a local ring with identity.
\item Let $\mathfrak{r}$ be the Jacobsen radical of $A$. Because $A$ is local, $\mathfrak{r}$ is maximal, and contains all non-units of $A$.
\item Let $*$ be an involution of $A$. Assume that elements fixed by $*$ are in the center of $A$, forming a ring $R = \{a \in A: a^* = a\}$.
Note that $R$ is local, with maximal ideal $R \cap \mathfrak{r}$. This is because any element of $R$ that is not in $R \cap \mathfrak{r}$ is invertible by definition.
Thus the only maximal ideal is $R \cap \mathfrak{r}$.
\item Let $Q: A^* \to R^*: a \mapsto aa^*$ (the norm map) be a group homomorphism with kernel $N$.
\end{itemize}

Now let $V$ be a right $A$-module and $h: V \times V \to A$ be a *-hermitian form.
By definition, $h$ is linear in the second variable and $h(v, u) = h(u, v)*$ for $u, v \in V$.
Then $h(u,u) = h(u,u)*$ and $h(u,u) \in R \subseteq Z(A)$ for all $u \in V$.

Now consider the dual space $V^*$. We can define an operation $V^* \times A \to V^*$ by $(\alpha a)(v) = a^* \alpha(v)$ where $\alpha \in V^*, a \in A, v \in V$.
Under this operation, $V^*$ is a right $A$-module.
Now we can define a homomorphism of right $A$-modules $\gamma_h: V \to V^*$ associated with $h$ given by $\gamma_h(u) = h(u, -)$.

% Should I show detailed calculation here?

Some more assumptions:

\begin{itemize}
\item Assume that $h$ is non-degenerate; $\gamma_h$ is an isomorphism.
\item Let $U$ be the subgroup of $GL(V)$ preserving $h$. I'm guessing that this means for $\varphi \in U, u,v \in V$, $h(\varphi(u), \varphi(v)) = h(u,v)$.
\item Assume the existence of an element $d \in A$ such that $d + d^* = 1$.
\item Assume that $V$ is a free $A$-module of rank $m \ge 1$. According to paper: "This is well defined, as can be seen by reducing modulo $\mathfrak{r}$.
\end{itemize}

Now let $\{v_1, v_2, \dotsc, v_m\}$ be a basis of $V$ throughout the section.

% Lemma 2.1
\begin{lemma}
There is a vector $u \in V$ such that $h(u,u) \in R^*$.
\end{lemma}
\begin{proof}
Assume otherwise; that for some $h(u,u) \in \mathfrak{m}$ for all $u \in V$.
Then using the linearity of $h$:
\[
h(u,v) + h(u,v)^* = h(u+v, u+v) - h(u,u) - h(v,v) \in \mathfrak{m}
\]
for all $u,v \in V$.
Let $\alpha \in V*$ be the linear functional such that $\alpha(v_1) = d$ and $\alpha(v_i) = 0$ for all $i > 1$.
Because $h$ is assumed to be non-degenerate, there exists $u \in V$ such that $h(u,-) = \alpha$.
Then $d = \alpha(v_1) = h(u,v_1)$ and $1 = d + d^* = h(u,v_1) + h(u,v_1)^* \not\in \mathfrak{m}$, contradicting the original hypothesis.
\end{proof}

% Lemma 2.2
\begin{lemma}\label{lemma2.2}
$V$ has an orthogonal basis $u_1, u_2, \dotsc u_m$.
Any such basis satisfies $h(u_i, u_i) \in R^*$.
\end{lemma}
\begin{proof}
Prove with induction on $m$.
Assume that $m = 1$.
By the previous lemma, there exists $u \in V$ such that $h(u,u) \in R^*$.
Then $u = v_1a_1$ for some $a_1 \in A^*$, and $h(u,u) = h(v_1a_1, v_1a_1) = a_1^*h(v_1,v_1)a_1 \in R*$ implying that $h(v_1, v_1) \in R^*$.
Now assume that $m > 1$ and that the hypothesis holds for $m - 1$.
Once again, there exists $u \in V$ such that $h(u,u) \in R^*$.
Then $u = v_1a_1 + \dotsb + v_m a_m$ with $a_i \in A$.
If all $a_i \in \mathfrak{r}$, then $h(u,u) \in \mathfrak{m}$, a contradiction.
Without loss of generality, assume that $a_1 \not\in \mathfrak{r}$.
Then if $u_1 = v_1 a_1$, the set $\{u_1, v_2, \dotsc, v_m\}$ is a basis of $V$.
For $1 < i \le m$, set
\[
u_i = v_i - u_1[h(u_1,v_i)/h(u_1, u_1)]
\]
Then $u_1, u_2, \dotsc, u_m$ is a basis of $V$ satisfying $h(u_1, u_i) = 0$ for $1 < i \le m$.
Let $V_1 = u_1 A$ and $V_2 = \text{span} \{u_2, \dotsc, u_m\}$.
Then $V = V_1 \perp V_2$ and the restriction of $h$ to $V_2$ induces an isomorphism $V_2 \to V_2^*$.
Applying the inductive hypothesis to this space completes the proof.
\end{proof}

\begin{lemma}
\begin{itemize}
% Come back to this to clean up the latex
\item{(a)} Suppose $u_1, \dotsc, u_s \in V$ are orthogonal and satisfy $h(u_i, u_i) \in R^*$.
Then $u_1, \dotsc , u_s \in V$ can be extended to an orthogonal basis of $V$ with the same property.
\item{(b)} If $V_1$ is a submodule of $V$ such that the restriction of $h$ to $V_1$ is non-degenerate there is another such submodule $V_2$ of $V$ such that $V = V_1 \perp V_2$.
\end{itemize}
\end{lemma}
\begin{proof}
(a) Can represent $u_1 = v_1 a_1 + \dotsb + v_m a_m$ for some $a_i \in A$.
Since $h(u_1, u_1) \in R^*$ (by previous lemma), one of the scalars must be a unit. WLOG assume $a_i \in A*$.
Thus $u_1, v_2, \dotsc, v_m$ is a basis of $V$.
Suppose $1 \le t \le s$ and the list $u_1, \dotsc, u_t, u_{t+1}, \dotsc, v_m$ is a basis of $V$.
Then 
\[
u_{t+1} = u_1b_1 + \dotsb + u_t b_t + v_{t+1} b_{t+1} + \dotsb + v_m b_m
\]
for some $b_i \in A$.
Suppose, if possible, that $b_i \in \mathfrak{r}$ for all $i \ge t+1$.
Then for every $i \le t$,
\[
0 = h(u_i, u_{t+1}) = h(u_i, u_i) b_i + h(u_i, v_{t+1})b_{t+1} + \dotsb + h(u_i, v_m) b_m
\]
implying that $b_i \in \mathfrak{r}$ for all $1 \le i \le t$, contradicting the assumptino that $h(u_{t+1}, u_{t+1}) \in R^*$.
Thus at least one of $b_{t+1}, \dotsc,b_m$ is a unit (assume $b_{t+1}$ and $u_1, \dotsc u_t, u_{t+1}, v_{t+2}, \dotsc, v_m$ is a basis of $V$.

This process can be repeated to extend $u_1, \dotsc, u_s$ to a basis $u_1, \dotsc, u_s, u_{s+1}, \dotsc u_m$ of $V$.
For $s < i \le m$, let
\[
z_i = u_i - ([u_1h(u_1, u_i)/h(u_1, u_1)] + \dotsb + u_s h(u_s, u_i) / h(u_s, u_s)].
\]
Then $u_1, \dotsc, u_s, z_1, \dotsc, z_{m-s}$ is a basis of $V$ satisfying $h(u_i, z_j) = 0$.
If follows that the restriction of $h$ to $M = \text{span} \{z_1, \dotsc, z_{m-s}\}$ is non-degenerate and by \cref{lemma2.2} that $M$ has an orthogonal basis with $h(z_i, z_i) \in R^*$ for any $i \le m - s$.

(b) Follows from (a) and \cref{lemma2.2}
\end{proof}

\begin{lemma}Let $u_1, \dotsc u_s \in V$, with corresponding Gram matrix $M \in M_s(A)$, defined by $M_{ij} = h(u_i, u_j)$.
If $M \in GL_m(A)$, then $u_1, \dotsc, u_s$ are linearly independent.
\end{lemma}
\begin{proof}
Suppose $a_1, \dotsc, a_s$ satisfy $u_1 a_1 + \dotsb + u_sa_s = 0$.
Then for $1 \le i \le s$
\[
0 = h(u_i, u_1 a_1 + \dotsb + u_s a_s) = h(u_i, u_1)a_1 + \dotsb + h(u_i, u_s)a_s
\]
implying that 
\[
M \left( \begin{array}{c}
a_1 \\
\vdots \\
c_1
\end{array} \right)
= \left( \begin{array}{c}
0 \\
\vdots \\
0
\end{array}
\right).
\]
Since $M$ is inverible, the desired result follows.
\end{proof}





\end{document}